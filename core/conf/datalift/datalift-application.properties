# Datalift application configuration
#
# Set the "datalift.home" Java VM system property (or the "DATALIFT_HOME"
# environment variable to point at the root directory of the Datalift
# installation
#
# ============================================================================
#
# Local installation paths
#
# The directory where to look for Datalift modules
datalift.modules.path           = ${datalift.home}/modules
#
# The private and public file system storage paths
datalift.private.storage.path   = ${datalift.home}/storage
datalift.public.storage.path    = ${datalift.private.storage.path}/public
#
# ============================================================================
#
# The RDF repositories
#
# Two repository names are recognized and reserved:
#  - data:      the public (i.e. published) RDF data, and
#  - internal:  the Datalift private data storage.
# All other names are available for module-specific repositories.
#
datalift.rdf.repositories       = data, internal, secured
#
data.repository.default         = true
#
# Sesame HTTP repositories
#
data.repository.url             = \
            http://localhost:8080/openrdf-sesame/repositories/lifted
#data.repository.username        = 
#data.repository.password        = 
#
internal.repository.url         = \
            http://localhost:8080/openrdf-sesame/repositories/internal
#internal.repository.username    = 
#internal.repository.password    = 
#
secured.repository.url          = \
	    http://localhost:8080/openrdf-sesame/repositories/secured
#secured.repository.username     = 
#secured.repository.password     = 
#
# Virtuoso repositories
#
#data.repository.url             = jdbc:virtuoso://localhost:1121
#data.repository.username        = dba
#data.repository.password        = dba
#data.repository.sparql.url      = http://localhost:8891/sparql
#
#internal.repository.url         = jdbc:virtuoso://localhost:1122
#internal.repository.username    = dba
#internal.repository.password    = dba
#internal.repository.sparql.url  = http://localhost:8892/sparql
#
# AllegroGraph repositories
#
#data.repository.url             = \
#        http://localhost:10035/catalogs/datalift/repositories/lifted
#data.repository.type            = allegrograph
#data.repository.username        = datalift
#data.repository.password        = test
#
#internal.repository.url         = \
#        http://localhost:10035/catalogs/datalift/repositories/internal
#internal.repository.type        = allegrograph
#internal.repository.username    = datalift
#internal.repository.password    = test
#
# ============================================================================
#
# SPARQL endpoint configuration
#
# The maximum duration of a SPARQL query, as a number of seconds.
# Any query taking longer to execute than this configured threshold
# will be aborted and partial results returned to the user.
# Defaults to -1 (no time limit).
#
#sparql.max.query.duration       = 30
#
# The predefined SPARQL queries definition file
# This RDF file (Turtle, N3, RDF/XML...) defines the predefined queries to
# make available on the SPARQL endpoint HTML user interface. Each query
# shall have <http://www.datalift.org/core#SparqlQuery> as RDF type, zero
# to N labels (<http://www.w3.org/2000/01/rdf-schema#label>) and the query
# string as RDF value (<http://www.w3.org/1999/02/22-rdf-syntax-ns#value>).
#
#sparql.predefined.queries.file  = ${datalift.home}/conf/predefined-queries.ttl
#
# Whether to use the Concise Bounded Description
# (http://www.w3.org/Submission/CBD/) feature provided by the back-end
# RDF store, to build RDF resource descriptions.
# Defaults to false, i.e. Datalift uses its own DESCRIBE query to include
# related blank nodes
#
#sparql.use.repository.cdb       = false
#
# Whether to include inferred triples when evaluating SPARQL queries.
# Defaults to true.
#
#sparql.include.inferred.triples = true
#
# The maximum number of results (triples, SPARQL results...) to
# return when outputting to HTML.
# Defaults to 1000.
#
#sparql.max.html.results         = 1000
#
# ============================================================================
#
# SPARQL security configuration using S4AC (Social Semantic SPARQL Security
# for Access Control) access control module
#
# List of protected RDF stores
# This property shall be defined for S4AC module to be active.
# Note: Datalift internal repository shall NEVER be included as the project
#       manager already enforces its own access control rules.
#
sparql.secured.repositories     = data
#
# Access control policies
# 1. Retrieving policies from an RDF store
# The following property defines the RDF store from which S4AC access control
# policies shall be read. The property value can be either the name of
# a RDF store defined in this configuration or a full URL.
#
#sparql.security.repository      = internal
#
# 2. Reading policies from local RDF files
# If no RDF store is available (property sparql.security.repository is
# undefined), access control policies can be loaded from RDF files.
#
#sparql.security.policy.files    = ${datalift.home}/conf/s4ac-policies.ttl
#
# Repository against which evaluating the ASK queries of access control
# policies. If undefined, the queries are evaluated against the RDF store
# targeted by the user SPARQL query.
#
sparql.security.user.repository.uri = secured
#
# Base URI to build the user context URI from the user's login. When
# evaluating access control policy ASK queries, the S4AC module uses
# this URI to populates the ?context variable.
#
#sparql.security.user.context    = http://www.datalift.org/security/user/{0}
sparql.security.user.context = http://example.com/context/{0}#ctx
#
# ============================================================================
#
# Configuration of web access to RDF resources
#
# Whether to serve RDF resources before checking for files in public storage
# when resolving URIs.
# Defaults to false, i.e. files are checked and served first.
#
#datalift.resources.rdf.first    = false
#
# The duration (in seconds) the client is allowed to keep a representation in
# its local cache when a resource is accessed during business opening hours.
# Defaults to 2 hours (7200 seconds).
#
#datalift.cache.duration         = 7200
#
# The business opening hours. If a resource is accessed outside this range,
# the representation expiry date is the start of the next business day.
# The business day can cross the day boundary (e.g. 22:00-06:00) in case
# updates are being performed by nightly batches.
# Defaults to: 08:00-18:00
#
#datalift.cache.businessDay      = 08:00-18:00
#
# ============================================================================
#
# Project manager configuration
#
# The license definition file
# This RDF file (Turtle, N3, RDF/XML...) defines the supported licenses for
# projects. Each definition shall have <http://www.datalift.org/core#License>
# as RDF type, a license URI (<http://purl.org/dc/terms/license>) and zero
# to N labels (<http://www.w3.org/2000/01/rdf-schema#label>).
#
#project.licenses.file           = ${datalift.home}/conf/licenses.ttl
#
# Batch size for RDF bulk operations
# Defaults to 10000.
#
datalift.rdf.batch.size         = 5000
#
# ============================================================================
#
# Security configuration
#
# Use simple properties file-based authentication for tests
propertiesRealm = org.apache.shiro.realm.text.PropertiesRealm
propertiesRealm.resourcePath = ${datalift.home}/conf/datalift-users.properties
#
# ============================================================================
#
